---
presentation:
  slideNumber: true
  width: 1280
  height: 720
---

<!-- slide -->
# Генерализация множеств точечных объектов

Картографические базы данных. Лекция 3

**Тимофей Самсонов**
[tsamsonov@geogr.msu.ru](tsamsonov@geogr.msu.ru)

<!-- slide -->
# Генерализация множеств точечных объектов

**Методы генерализации**

1. Регионизация
1. Отбор
1. Кластеризация

**Аспекты**

1. Вспомогательные геометрические структуры
1. Параметризация алгоритмов
1. Оценка результатов

<!-- slide -->
# Вспомогательные геометрические структуры

<!-- slide -->
# Триангуляция Делоне и диаграмма Вороного

**Триангуляция** (на плоскости) — это разбиение фигуры на треугольники.

<div style="display: inline-block; width: 65%; vertical-align: top;">

- Треугольники покрывают всю фигуру без «дыр», при этом они либо не пересекаются, либо касаются в точке или вершине.

- Под триангуляцией множества точек понимается *триангуляция их выпуклой оболочки*, в которой сами точки являются вершинами треугольников.

- В *триангуляции Делоне* окружность, описанная вокруг каждого треугольника, не содержит других точек этого множества

</div>

<div style="display: inline-block; width: 30%; vertical-align: top; margin-left: 2%">
  ![](tin.svg){width=300px}
</div>

**Delaunay, Boris** (1934). *Sur la sphère vide*. Bulletin de l'Académie des Sciences de l'URSS, Classe des sciences mathématiques et naturelles. 6: 793–800.

<!-- slide -->
# Триангуляция Делоне и диаграмма Вороного

**Диаграмма Вороного** конечного множества точек $S$ на плоскости представляет такое разбиение плоскости, при котором каждая область этого разбиения образует множество точек, более близких к одному из элементов множества $S$, чем к любому другому элементу множества.

<div style="display: inline-block; width: 55%; vertical-align: top;">

- Другие названия диаграммы — *полигоны Тиссена*, *разбиение Дирихле*

- Диаграмма Вороного строится на основе серединных перпендикуляров к сторонам треугольников

- Диаграмма Вороного множества точек взаимно однозначна триангуляции Делоне множества точек

</div>

<div style="display: inline-block; width: 40%; vertical-align: top; margin-left: 2%">
  ![](voronoi.svg){width=300px}
</div>

**Voronoi, Georgy** (1908). *Nouvelles applications des paramètres continus à la théorie des formes quadratiques*. Journal für die Reine und Angewandte Mathematik. 133 (133): 97–178. doi:10.1515/crll.1908.133.97.

<!-- slide -->
# Диаграмма Вороного

Диаграмма Вороного представляет из себя способ моделирования конкурентных зон

![https://en.wikipedia.org/wiki/Voronoi_diagram](voronoi.gif){width=450px}

<!-- slide -->
# Диаграмма Вороного

Для вычисления диаграммы могут быть использованы различные метрики

<div style="display: inline-block; width: 45%; vertical-align: top;">
  **Евклидово расстояние**

  $d_{12} = \sqrt{(x_1-x_2)^2 + (y_1 - y_2)^2}$

  ![](voronoi_std.svg)
</div>

<div style="display: inline-block; width: 45%; vertical-align: top; margin-left: 2%">
  **Манхэттенское расстояние**

  $d_{12} = |x_1-x_2| + |y_1 - y_2|$

  ![](voronoi_mnh.svg)
</div>

<!-- slide -->
# Регионизация точек

<!-- slide -->
# Регионизация точек

**Методы регионизации:**

1. Ограничивающие прямоугольники
2. Выпуклая оболочка
3. Невыпуклые оболочки ($\alpha$-shape, $\chi$-shape)

<!-- slide -->
# Ограничивающий прямоугольник (envelope)

![](envelope.png){width=800px}

- Наиболее простой тип оболочки, который используется в ГИС для описания территориального охвата слоя (*экстент*)

- Для построения в общем случае нужно 4 точки, которые обладают минимальным и максимальным значением координат из всего множества

<!-- slide -->
# Выпуклая оболочка (convex hull)

![](chull.png){width=500px}

- **Выпуклой оболочкой** множества $p$ называется наименьшее выпуклое множество, содержащее $p$

- Выпуклая оболочка обозначается как $Conv(p)$

- Выпуклая оболочка на плоскости является *пересечением всех полуплоскостей, содержащих* $p$

- Существует несколько алгоритмов построения $Conv(p)$

<!-- slide -->
# Выпуклая оболочка
<div style="display: inline-block; width: 45%; vertical-align: top;">
  **Аналогия**

  Представьте себе лассо, которое накидывается на множество гвоздей,  битых в доску:

  ![](lasso.png){width=330px}
</div>

<div style="display: inline-block; width: 45%; vertical-align: top; margin-left: 2%">
  **Выпуклое и невыпуклое**

  *Выпуклое множество* в аффинном или векторном пространстве — множество, содержащее вместе с любыми двумя точками соединяющий их отрезок:

  ![](convex_concave.png)
</div>

<!-- slide -->
# Выпуклая оболочка

**Алгоритм Джарвиса**

<div style="display: inline-block; width: 55%; vertical-align: top;">

  1. Найти точку $p_0$ с наименьшим значением координаты $X$. Добавить в результат.
  2. Найти точку $p_1$, образующую наименьший угол поворота по часовой стрелке относительно направления оси $Y$. Добавить в результат.
  3. Найти все оставшиеся точки выпуклой оболочки $p_i, i = 2..k$, выбирая каждый раз такую точку $p_i$, что угол поворота от $p_{i-2} p_{i-1}$ к $p_{i-2} p_{i-1}$ наименьший. Остановиться, когда $p_i = p_0$

</div>

<div style="display: inline-block; width: 40%; vertical-align: top; margin-left: 2%">
  ![](jarvis.png)
</div>

**Jarvis, R. A.** (1973). *On the identification of the convex hull of a finite set of points in the plane*. Information Processing Letters 2: 18–21. doi:10.1016/0020-0190(73)90020-3.


<!-- slide -->
# Выпуклая оболочка

**Алгоритм Джарвиса**

![](jarvis.gif)

**Jarvis, R. A.** (1973). *On the identification of the convex hull of a finite set of points in the plane*. Information Processing Letters 2: 18–21. doi:10.1016/0020-0190(73)90020-3.

<!-- slide -->
# Минимальный по площади ограничивающий прямоугольник

**Минимальный по площади ограничивающий прямоугольник** (*minimum bounding rectangle --- MBR*) является наименьшим по площади среди всех прямоугольников, охватывающих данное множество точек

<div style="display: inline-block; width: 45%; vertical-align: top;">
  ![](mbr.png)
</div>

<div style="display: inline-block; width: 45%; vertical-align: top; margin-left: 2%">
  Согласно теореме Фримана-Шапиро *одна из сторон MBR должна касаться выпуклой оболочки*.

  **Freeman, H.; Shapira, R.** (1975), *Determining the minimum-area encasing rectangle for an arbitrary closed curve*. Communications of the ACM, 18: 409–413, MR 0375828, doi:10.1145/360881.360919.
</div>


<!-- slide -->
# Алгоритм Rotating Calipers

Поворачиваем ограничивающий прямоугольник, совмещая его с одной из сторон выпуклой оболочки. Из них выбираем минимальный по площади.

![](calipers.png){width=700px}

**G.T.Toussaint** (1983). *Solving geometric problems with the rotating calipers*. Proceedings of IEEE MELECON'83, Athens, Greece, May 1983, pp. A10. 02/1-4.

<!-- slide -->
# Альфа-оболочка

**Альфа-оболочка ($\alpha$-shape)** --- обобщение понятия выпуклой оболочки, позволяющее строить оболочки, более точно описывающие форму множества, в том числе и вогнутые оболочки. Альфа-оболочка является площадной компонентой *альфа-комплекса ($\alpha$-complex)*, который строится на основе триангуляции Делоне множества точек.

![](ashape.png){width=600px}

**Edelsbrunner H, Kirkpatrick D, Seidel R **(1983) *On the shape of a set of points in the plane*. IEEE Transactions on Information Theory, 29:551–559. doi: 10.1109/TIT.1983.1056714

<!-- slide -->
# Альфа-комплекс

**Альфа-комплекс ($\alpha$-complex)** --- подмножество триангуляции Делоне, включающее узлы, а также ребра и треугольники, существующие при заданном радиусе диска $r= 1/\alpha$ (при $\alpha > 0$).

![](acomplex.png){width=900px}

- При увеличении радиуса диска происходит образование и исчезновение циклов.

- При $\alpha = 0$ объединение треугольников альфа-комплекса совпадает с *выпуклой оболочкой* множества точек.

<!-- slide -->
# Типы альфа-комплексов
<div style="display: inline-block; width: 45%; vertical-align: top;">
  **Стандартный**

  $\alpha = const$

  ![](acomplex_std.png)
</div>

<div style="display: inline-block; width: 45%; vertical-align: top; margin-left: 2%">
  **Взвешенный**

  $\alpha = f(p)$

  ![](acomplex_wgt.png)
</div>

<!-- slide -->
# Отбор точек

<!-- slide -->
# Отбор точек

**Алгоритмы отбора:**

1. Разделения населенных пунктов (*settlement-spacing*)

1. Растущей окружности (*circle-growth*)

1. Взвешенной эффективной площади (*effective area*)

**Все алгоритмы** используют весовые коэффициенты для оценки важности точке. В качестве аргумента весовой функции могут быть использованы:

- значение показателя в точке
- статус (ранг) точки
- расстояние до ближайшей точки
- эффективная площадь точки (ячейка Вороного)

<!-- slide -->
# Алгоритм Settlement-spacing

Вокруг каждой точки строится окружность радиуса $R_i = C/W_i$, где $W_i$ — вес точки, а $С$ — константа. Точки сортируются в порядке уменьшения $W_i$.

1. Выбирается первая точка. Добавляется в результат.

1. Выбирается следующая точка. Если она содержит в окружности ранее добавленную точку, она исключается. Если нет, то добавляется в результат.

2. Шаг 2 повторяется, пока не будет достигнут требуемый процент отбора точек, либо не исчерпается множество точек.

![](stl-spacing.png)

**Langran C. E. and Poiker T. K.** *Integration of name selection and name placement* // Proceedings of II International Symposium on Spatial Data Handling, 1986, pp. 50–64.

<!-- slide -->
# Алгоритм Circle-growth

Радиус окружности $R = C \times W_i$ прямо пропорционален весу точки $W_i$.

1. Начальное значение $С$ выбирается таким образом, чтобы окружность наиболее важной точки не накрывала ни одну соседнюю точку.

1. Значение $С$ увеличивается, при этом удаляются те точки, окружности которых оказались целиком внутри окружности точки более высокого ранга.

1. Процедура продолжается до тех пор пока, не будет достигнут требуемый процент удаляемых точек.

![](circ-growth.png)

**van Kreveld, M., van Oosterom, R., and Snoeyink, J.** *Efficient settlement selection for interactive display*, in Proceedings of AutoCarto 12, Bethesda, MD, 1995, pp. 287–296.

<!-- slide -->
# Алгоритм эффективной площади

1. Построить оболочку точек (convex hull, $\alpha$-shape)

2. Построить диаграмму Вороного точек в пределах оболочки.

3. Вычислить веса каждой точки обратно пропорционально площади ее ячейки Вороного $W_i = A_i^{-1}$. Пометить все точки как активные.

4. Удалить активную точку с наименьшим весом.

5. Перестроить локально диаграмму Вороного для соседей точки первого порядка. Деактивировать соседей (запретить их удаление).

6. Повторять шаги 4-5 для оставшихся точек, пока не будут деактивированы все точки или пока не будет достигнут требуемый процент отбора.

7. Если не достигнут требуемый процент отбора точек, активировать все точки и перейти к шагу 4. В противном случае завершить обработку

**Ai T., Liu Y.**  *A method of point cluster simplification with spatial distribution properties preserved*, Acta Geodaetica et Cartographica Sinica, 25(1), 35–41, 2002.

<!-- slide -->
# Алгоритм эффективной площади

<div style="display: inline-block; width: 45%; vertical-align: top;">
  **До:**

  ![](effa_source.png)
</div>

<div style="display: inline-block; width: 45%; vertical-align: top; margin-left: 2%">
  **После:**

  ![](effa_result.png)
</div>

**Ai T., Liu Y.**  *A method of point cluster simplification with spatial distribution properties preserved*, Acta Geodaetica et Cartographica Sinica, 25(1), 35–41, 2002.

<!-- slide -->
# Алгоритм эффективной площади

**Локальное перестроение диаграммы Вороного:**

![](effa_recalc.png)

- выбрать соседей первого и второго порядка, построить для них диаграмму Вороного

- выбрать полигоны Вороного соседей первого порядка и заменить ими полигоны Вороного в исходной диаграмме

**Деактивация** соседей первого порядка позволяет избежать прореживания множества точек в местах их значительного скопления

<!-- slide -->
# Алгоритм взвешенной эффективной площади

Вес точки масштабируется пропорционально ее значимости, а не только эффективной площади: $W_i = С_i \times A_i^{-1}$, где параметр $С_i$ можно представить как средневзвешенную сумму баллов за факторы:

$$
С_i = \frac{\sum w_j f_{ij}}{\sum w_j}
$$

где $w_j$ – вес $j$-го фактора, $f_{ij}$ — значение $j$-го фактора в $i$-й точке.

**Yan, H. W and Li, Z. L.**, *A Voronoi-based algorithm for point cluster generalization* // Proceedings of 11th International Conference on Geometry and Graphics, August 2004, Guangzhou, 2004.

**Samsonov T., Krivosheina A.**  *Joint generalization of city points and road network for small-scale mapping* // Proceedings of Seventh International Conference on Geographic Information Science GIScience, September 18-21, 2012. — Columbus, Ohio, 2012. — P. 1–7.

<!-- slide -->
# Районирование

Можно выполнить районирование территории по густоте размещения и для каждого района устанавливать свои допуски и критерии генерализации

![](sel_regions.png){width=700px}

**Самсонов Т. Е., Кривошеина А. М.** *Автоматизация отбора населенных пунктов с учетом пространственной неравномерности их распределения для целей мелкомасштабного картографирования* // Известия высших учебных заведений. Геодезия и аэрофотосъемка. — 2015. — № 1. — С. 74–82.

<!-- slide -->
# Районирование

Можно выполнить районирование территории по густоте размещения и для каждого района устанавливать свои допуски и критерии генерализации

![](sel_regions2.png){width=700px}

**Самсонов Т. Е., Кривошеина А. М.** *Автоматизация отбора населенных пунктов с учетом пространственной неравномерности их распределения для целей мелкомасштабного картографирования* // Известия высших учебных заведений. Геодезия и аэрофотосъемка. — 2015. — № 1. — С. 74–82.

<!-- slide -->
# Графическая оценка результатов

**До генерализации:**

![](graph_est_src.png)

1 у.е. — пунсон населенного пункта наименьшего класса

<!-- slide -->
# Графическая оценка результатов

**После генерализации:**

![](graph_est_dest.png)

1 у.е. — пунсон населенного пункта наименьшего класса

<!-- slide -->
# Численная оценка результатов

**Закон квадратного корня:**  *количество объектов уменьшается пропорционально квадратному корню отношения исходного и результирующего масштабов* (Topfer, Pillewizer, 1966). В более общем виде закон можно сформулировать следующим образом:

$$
N_T = N_S \sqrt {\left(\frac{S_S}{S_T}\right)^x}
$$

где $N_T$ --- количество объектов в целевом масштабе, $N_S$ --- количество объектов в исходном масштабе, $S_T$ --- целевой масштаб, $S_S$ --- исходный масштаб. Параметр $x$ определяет степень отбора при генерализации и принимает следующие характерные значения:

- 0 — нет отбора
- (0, 4) — *увеличение* густоты
- 4 — *сохранение* густоты
- \> 4 — *уменьшение* густоты

**Topfer, F. and Pillewizer, W.** (1966) *The Principles of Selection*. The Cartographic Journal, 3, 10-16. http://dx.doi.org/10.1179/caj.1966.3.1.10

<!-- slide -->
# Численная оценка результатов

Оценка по закону Топфера показывает, что, как правило, при уменьшении масштаба наблюдается выравнивание плотности объектов по различным районам, а также общее уплотнение объектов:

<div style="display: inline-block; width: 45%; vertical-align: top;">

Класс | Значение параметра $x$
------|-----------------------
1  | 4,2
2  | 3,0
3  | 2,0
4  | 1,2
Всего  | 3,8

</div>

<div style="display: inline-block; width: 50%; vertical-align: top; margin-left: 2%">

![](sel_regions2.png){width=500px}

</div>


<!-- slide -->
# Кластеризация

<!-- slide -->
# Постановка задачи кластеризации

**Дано**:

- $X$ --- пространство объектов;
- $X^l$ = $\{x_1, ..., x_l\}$ --- обучающая выборка;
- $\rho: X \times X \rightarrow [0, \infty)$ --- функция расстояния между объектами.

**Найти:**

- $Y$ --- множество кластеров;
- $a: X \rightarrow Y$ --- алгоритм кластеризации,

*Такие что*:

  - каждый кластер состоит из близких объектов
  - объекты разных кластеров существенно различны

Кластеризация относится к методам классификации (обучения) *без учителя*

**К.В.Воронцов** Машинное обучение (курс лекций). [http://www.machinelearning.ru](http://www.machinelearning.ru)

<!-- slide -->
# Кластеризация

**Цели кластеризации:**

1. Упрощение обработки данных
1. Сжатие данных
1. Выделение аномалий (нетипичных объектов)
1. Построение иерархии объектов

**Проблемы кластеризации:**

1. Отсутствие точной постановки задачи
1. Разнообразие критериев качества кластеризации
2. Распространенность эвристических подходов
3. Неизвестное количество кластеров
4. Зависимость результата от выбранной метрики

<!-- slide -->
# Алгоритмы кластеризации

1. Статистическая кластеризация

    - К средних
    - ISODATA
    - ФОРЭЛ (формальные элементы)

1. Иерархическая/графовая кластеризация

1. Плотностная кластеризация (DBSCAN/OPTICS)

<!-- slide -->
# Метод К средних

1. Установить количество кластеров $K$
2. Выбрать случайным образом $K$ точек в качестве центров кластеров
3. Определить для каждой точки ближайший центр кластера
4. Рассчитать новый центр кластера на основе координат его точек
5. Повторять шаги 3-4, до тех пор пока центры кластеров не перестанут менять свое местоположение.

**Steinhaus H.** (1956). *Sur la division des corps materiels en parties.* Bull. Acad. Polon. Sci., C1. III vol IV: 801—804.

**Lloyd S.** (1957). *Least square quantization in PCM’s.* Bell Telephone Laboratories Paper.

**MacQueen J.** (1967). *Some methods for classification and analysis of multivariate observations.* In Proc. 5th Berkeley Symp. on Math. Statistics and Probability, pages 281—297.

<!-- slide -->
# Метод К средних
Пример с четырьмя кластерами:

![](kmeans.gif){width=600px}

[http://shabal.in/visuals/kmeans/](http://shabal.in/visuals/kmeans/)

<!-- slide -->
# Метод К средних

**Свойства метода**

Метод К средних стремится минимизировать суммарное квадратичное отклонение точек кластеров от центров этих кластеров:
  $$
  V = \sum_{i=1}^{k} \sum_{x_j \in S_I} (x_j - \mu_i)^2
  $$
где $k$ --- число кластеров, $S_i$ --- полученные кластеры, $\mu_i$ --- центры кластеров (центры масс векторов этих кластеров)

**Однако**: не гарантируется достижение глобального минимума суммарного квадратичного отклонения $V$, а только одного из локальных минимумов.

<!-- slide -->
# Метод К средних

**Свойства метода**

Все точки результирующих кластров лежат в пределах ячеек диаграммы Вороного для центров этих кластеров:

![](kmeans_vor.png){width=400px}

[https://moderndata.plot.ly/voronoi-diagrams-in-plotly-and-r/](https://moderndata.plot.ly/voronoi-diagrams-in-plotly-and-r/)

<!-- slide -->
# Метод К средних

**Свойства метода**

Результат зависит от выбора исходных центров кластеров, их оптимальный выбор неизвестен:

![](kmeans-1.png){width=700px}

![](kmeans-2.png){width=700px}

[E.M. Mirkes, K-means and K-medoids applet. University of Leicester, 2011.](http://www.math.le.ac.uk/people/ag153/homepage/KmeansKmedoids/Kmeans_Kmedoids.html)

<!-- slide -->
# Метод К средних

**Свойства метода**

Алгоритм хорошо выделяет только разнесенные в пространстве кластеры выпуклой формы. Пример неудачной кластеризации:

![](kmeans-irr.png){width=500px}


<!-- slide -->
# Метод К средних

**Свойства метода:**

1. Не гарантируется достижение глобального минимума суммарного квадратичного отклонения $V$, а только одного из локальных минимумов.

2. Результат зависит от выбора исходных центров кластеров, их оптимальный выбор неизвестен.

3. Алгоритм хорошо выделяет только кластеры выпуклой формы.

4. Метод может выявлять только заданное количество кластеров, но не их естественное число.

<!-- slide -->
# Метод ISODATA

**ISODATA** означает *Iterative Self-Organizing Data Analysis Technique Algorithm*

Метод начинает работу с одного кластера и выполняет рекурсивное разделение множества вдоль его наиболее протяженной оси до тех пор пока все внутрикластерные расстояния не будут в пределах заданного допуска.

**Ball, Geoffrey H., Hall, David J.** (1965) *Isodata: a method of data analysis and pattern classification* Stanford Research Institute, Menlo Park,United States. Office of Naval Research. Information Sciences Branch

<!-- slide -->
# Метод ISODATA

**Алгоритм:**

1. Установить допустимые значения стандартных отклонений $\sigma_x^{max}$, $\sigma_y^{max}$, минимальное расстояние между кластерами $d_max$ (опционально) и требуемое количество кластеров $K$ (опционально).
1. Распределить $k$ случайных центров кластеров. Допустимо принять $k=1$, тогда местоположение кластера не имеет значения.
1. Распределить точки в кластеры по расстоянию до ближайшего центра.
1. Вычислить расстояния $d$ между центрами кластеров. Если среди них есть такие что $d < d_{max}$, произвести объединение центров, вычислив координаты, взвешенные на количество точек, содержащихся в кластере. Заново перераспределить между ними точки.
1. Для каждого кластера вычислить значения выборочного среднего $\mu_x, \mu_y$ и стандартного отклонения $\sigma_x$ и $\sigma_y$ координат по осям $X$ и $Y$.
1. Разбить пополам значением $\mu_x$ или $\mu_y$ все кластеры, в которых $\sigma_x > \sigma_x^{max}$ или $\sigma_y > \sigma_y^{max}$. Если в кластере $\sigma$ превышает допуск по обоим направлениям, он разбивается по направлению, в котором $\sigma$ больше.
1. Вычислить центры разбитых кластеров, включить их в общее множество центров.
1. Повторять шаги 3-5, пока во всех кластерах значения $\sigma_x$ и $\sigma_y$ на станут меньше соответствующих значений $\sigma_x^{max}$ и $\sigma_y^{max}$ или когда не будет достигнуто требуемое количество кластеров $K$.

<!-- slide -->
# Метод ISODATA

![**Исходный набор данных** (Ball, Hall, 1965)](isodata-0.png){width=450px}

Вес и рост спортсменов из разных видов спорта

<!-- slide -->
# Метод ISODATA

![**Размещение исходных центров** (Ball, Hall, 1965)](isodata-1.png){width=500px}

Даже если центры выбраны неудачно, в результате они разместятся в подходящих местах.

<!-- slide -->
# Метод ISODATA

![**Разбиение диаграммой Вороного** (Ball, Hall, 1965)](isodata-2.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Распределение точек по кластерам** (Ball, Hall, 1965)](isodata-3.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Вычисление средней точки** (Ball, Hall, 1965)](isodata-4.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Разделение центров** (Ball, Hall, 1965)](isodata-5.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Разбиение диаграммой Вороного** (Ball, Hall, 1965)](isodata-6.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Распределение точек по кластерам** (Ball, Hall, 1965)](isodata-7.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Центры кластеров** после итерации 2 (Ball, Hall, 1965)](isodata-8.png){width=500px}

Точка-выброс помечена стрелочкой

<!-- slide -->
# Метод ISODATA

![**Центры кластеров** после итерации 3 (Ball, Hall, 1965)](isodata-9.png){width=500px}

Точка-выброс образовала самостоятельный кластер

<!-- slide -->
# Метод ISODATA

![**Разбиение кластеров** (Ball, Hall, 1965)](isodata-10.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Распределение точек по кластерам** (Ball, Hall, 1965)](isodata-11.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Вычисление новых центров кластеров** (Ball, Hall, 1965)](isodata-12.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Слияние близких центров** (Ball, Hall, 1965)](isodata-13.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Разбиение на кластеры** (Ball, Hall, 1965)](isodata-14.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Центры кластеров** (Ball, Hall, 1965)](isodata-15.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Центры кластеров и их слияние** (Ball, Hall, 1965)](isodata-16.png){width=500px}

<!-- slide -->
# Метод ISODATA

![**Итоговые центры** (Ball, Hall, 1965)](isodata-17.png){width=500px}

<!-- slide -->
# Метод ISODATA

**По сравнению с К средних:**

1. Не обязательно задавать количество кластеров.

2. Более робастный метод вычисления итоговых кластеров.

3. Точно так же хорошо справляется только с изометричными кластерами, однако по прежнему не эффективен при выделени кластеров неправильной формы

<!-- slide -->
# FOREL (ФОРмальные ЭЛементы)

**Параметры: **

- $U:= X^l$ --- множество некластеризованных точек
- $K$ --- количество кластеров
- $R$ --- радиус поиска.

**Алгоритм:**

1. Выбрать случайную точку $p_0$
2. Образовать кластер с центром в $p_0$ и радиусом $R$
3. Переместить центр $p_0$ в центр масс кластера
4. Повторять шаги 2-3, пока состав кластера $K_0$ не стабилизируется
5. Исключить кластеризованные точки из множества $U := K \setminus K_0$
6. Повторять шаги 2-5, пока в выборке есть некластеризованные точки

**Ёлкина В.Н., Ёлкин Е.А., Загоруйко Н.Г.** *О применении методики распознавания образов к решению задач палеонтологии*, 1967

<!-- slide -->
# FOREL (ФОРмальные ЭЛементы)

**Преимущества: **

- Точность минимизации функционала качества (при удачном подборе параметра R)
- Наглядность визуализации кластеризации
- Сходимость алгоритма
- Возможность операций над центрами кластеров — они известны в процессе работы алгоритма
- Возможность подсчета промежуточных функционалов качества, например, длины цепочки локальных сгущений
- Возможность проверки гипотез схожести и компактности в процессе работы алгоритма

**Недостатки: **

- Относительно низкая производительность (решается введением функции пересчета поиска центра при добавлении 1 объекта внутрь сферы)
- Плохая применимость алгоритма при плохой разделимости выборки на кластеры
- Неустойчивость алгоритма (зависимость от выбора начального объекта)
- Произвольное по количеству разбиение на кластеры
- Необходимость априорных знаний о ширине (диаметре) кластеров

<!-- slide -->
# Иерархическая кластеризация

**Алгоритм** (Lance, Williams, 1967) основан на пересчете расстояний $R_{UV}$ между кластерами $U$ и $V$:

1. Представить все кластеры как одноэлементные: $C_1 := \Bigl\{ \{x_1\}, ..., \{x_l\} \Bigr\}$.

1. Рассчитать расстояния между ними $R_{\{x_i\}\{x_j\}} = \rho_{x_i, x_j}$.

2. Установить переменную цикла $t = 2$.

2. Найти пару кластеров с минимальным расстоянием $\rho_{ij}$.

3. Объединить их в один кластер: $W := U \cup V$; $C_t := C_{t-1} \{W\} \setminus \{U, V\}$.

4. Пересчитать расстояния $R_{WS}$ от нового кластера $W$ до всех кластеров $S \in С_t$ по формуле Ланса-Уильямса: $R_{WS} := \alpha_{U}R_{US} + \alpha_{V}R_{S} + \beta R_{UV} + \gamma |R_{US} - R_VS|$.

5. Установить $t = t + 1$.

6. Повторять шаги 4-7, пока $t \leq l$.

**Lance G. N., Willams W. T.** *A general theory of classification sorting strategies. 1. hierarchical systems* // Comp. J. — 1967. — no. 9. — Pp. 373–380

<!-- slide -->
# Варианты расстояний
![](hierch-dist1.png){width=900px}

**К.В.Воронцов** Машинное обучение (курс лекций). [http://www.machinelearning.ru](http://www.machinelearning.ru)

<!-- slide -->
# Варианты расстояний
![](hierch-dist2.png){width=900px}

**К.В.Воронцов** Машинное обучение (курс лекций). [http://www.machinelearning.ru](http://www.machinelearning.ru)

<!-- slide -->
# Дендрограммы

**Расстояние ближнего соседа**

![](hierch-dend1.png){width=900px}

**К.В.Воронцов** Машинное обучение (курс лекций). [http://www.machinelearning.ru](http://www.machinelearning.ru)

<!-- slide -->
# Дендрограммы

**Расстояние дальнего соседа**

![](hierch-dend2.png){width=900px}

**К.В.Воронцов** Машинное обучение (курс лекций). [http://www.machinelearning.ru](http://www.machinelearning.ru)

<!-- slide -->
# Дендрограммы

**Групповое среднее**

![](hierch-dend3.png){width=900px}

**К.В.Воронцов** Машинное обучение (курс лекций). [http://www.machinelearning.ru](http://www.machinelearning.ru)

<!-- slide -->
# Дендрограммы

**Расстояние Уорда**

![](hierch-dend4.png){width=900px}

**К.В.Воронцов** Машинное обучение (курс лекций). [http://www.machinelearning.ru](http://www.machinelearning.ru)

<!-- slide -->
# Иерархическая кластеризация

1. Рекомендуют пользоваться расстоянием Уорда

1. Имеет смысл построить несколько вариантов и выбрать лучший

1. Уровень кластеризации получается путем отсечения дерева по заданному расстоянию.

![](hierch-res.png){width=800px}

**К.В.Воронцов** Машинное обучение (курс лекций). [http://www.machinelearning.ru](http://www.machinelearning.ru)

<!-- slide -->
# Кратчайший незамкнутый путь

**Алгоритм:**

1. Пометить все точки как изолированные
1. Найти пару вершин с наименьшим расстоянием и соединить их ребром. Пометить вершины как неизолированные
2. Найти изолированную точку, ближайшую к некоторой неизолированной
3. Соединить эти точки ребром
4. Повторять шаги 2-3, пока остаются изолированные точки
5. Удалить K-1 самых длинных рёбер.

**Недостатки**:

- необходимость задавать число кластеров
- высокая чувствительность к шуму.

**К.В.Воронцов** Машинное обучение (курс лекций). [http://www.machinelearning.ru](http://www.machinelearning.ru)

<!-- slide -->
# DBSCAN

**DBSCAN** *(Density-based spatial clustering of applications with noise)* --- алгоритм плотностной кластеризации, который группирует плотно упакованные точки.

<div style="display: inline-block; width: 45%; vertical-align: top;">
  Выделяются точки трех видов:

  1. Точка $p$ является *ядерной (core)*, если ее $\epsilon$-окрестность содержит не менее $minPts$ точек. Эти точки именуются *прямо достижимыми* от $p$
  2. Точка $q$ является *достижимой (reachable)* от точки $p$, если существует такая последовательность точек $p_1, ..., p_n$, $p_1 = p$, $p_n = q$ в которой каждая точка $p_{i+1}$ прямо достижима от $p_i$. Все точки, кроме $q$ должн быть ядерными
  3. Точка $r$ является *выброшенной (outlier)*, если оне не достижима ни от одной друго точки

</div>

<div style="display: inline-block; width: 45%; vertical-align: top; margin-left: 2%">
  Если $A$ --- ядерная точка, то она формирует кластер со всеми точками, которые достижимы от нее.  Точки $B$ и $С$ являются граничными (edge), поскольку входят в кластер, но при этом не являются ядерными. Точка $N$ выброшена:

  ![](dbscan.svg){width=300px}
  [https://en.wikipedia.org/wiki/DBSCAN](https://en.wikipedia.org/wiki/DBSCAN)
</div>

**Ester, Martin, Hans P Kriegel, Jorg Sander, and Xiaowei Xu**. 1996. *A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.* Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, 226–31. doi:10.1.1.71.1980.

<!-- slide -->
# DBSCAN

**Алгоритм:**

1. Задать С = 0
1. Для каждой точки $p$ из множества $P$:
    i. Найти множество точек $N$, входящих в $\epsilon$-окрестность точки $p$
    i. Если $|N| < minPts$, то пометить точку $p$ как выброшенную и перейти на новую итерацию.
    i. Если $|N| \geq minPts$, то создать новый номер кластера $C = C+1$ и пометить им точку $p$
    i. Создать множество точек $Q = N \setminus \{p\}$
    i. Для каждой точки $q$ из множества $Q$:
        1. Если $q$ помечена как выброшенная, пометить $q$ номером кластера $C$
        1. Если $q$ имеет метку, перейти на новую итерацию.
        1. Если $q$ не имеет метки, то:
            i. пометить $q$  номером кластера $C$
            ii. найти множество точек $K$, входящих в $\epsilon$-окрестность $q$
            iii. если $|K| \geq minPts$, то $S = S \cup K$ (добавить $K$ к $S$)

**Ester, Martin, Hans P Kriegel, Jorg Sander, and Xiaowei Xu**. 1996. *A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.* Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, 226–31. doi:10.1.1.71.1980.

<!-- slide -->
# DBSCAN

**Пуассоновский (случайный) точечный процесс:**

![](dbscan_uniform.gif)

*Visualizing DBSCAN Clustering:*

[https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

<!-- slide -->
# DBSCAN

**Кластерный процесс Томаса (смесь нормальных распределений):**

![](dbscan_gaussian.gif)

*Visualizing DBSCAN Clustering:*

[https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

<!-- slide -->
# DBSCAN

**Смайлик:**

![](dbscan_face.gif)

*Visualizing DBSCAN Clustering:*

[https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

<!-- slide -->
# DBSCAN

<div style="display: inline-block; width: 45%; vertical-align: top;">

**Достоинства:**

1. *DBSCAN* не требует указания количества кластеров.

1. *DBSCAN* может найти кластеры произвольной формы. Он может даже найти кластер, полностью окруженный другим кластером, но не связанный с ним.

1. *DBSCAN* имеет представление о шуме и устойчив к выбросам.

1. *DBSCAN* требует всего два параметра и в основном нечувствителен к упорядочению точек в исходном множестве. Тем не менее, точки, расположенные на краю двух разных кластеров, могут менять кластерное членство.

1. *DBSCAN* способен использовать пространственные индексы для эффективного поиска точек в окрестности.

</div>

<div style="display: inline-block; width: 45%; vertical-align: top; margin-left: 2%">

**Недостатки:**

1. *DBSCAN* не является полностью детерминированным: граничные точки, доступные из более чем одного кластера, могут быть частью любого кластера, в зависимости от порядка обработки данных.

1. Качество *DBSCAN* зависит от метрики расстояния, используемой в функции запроса диапазона (*region query*). Наиболее распространенной метрикой расстояния является евклидово расстояние, однако в пространствах высокой размерности подбор подходящего значения $\epsilon$ вызывает большие трудности.

1. *DBSCAN* не может хорошо сгруппировать наборы данных в которых кластеры имеют разную плотность. Комбинация $minPts$ и $\epsilon$ характеризует только определенный масштаб кластеризации.

</div>

<!-- slide -->
# DBSCAN

**Выбор параметров**

1. Для определения оптимальной величины $\epsilon$ рекомендуется построить функцию среднего расстояния до $minPts-1$ ближайших соседей в зависимости от $\epsilon$. Оптимальными считаются значения $\epsilon$, при которых наблюдается наиболее интенсивное изменение роста функции (перегиб):

![](dbscan_eselect.png){width=600px}

**Sander, Jörg; Ester, Martin; Kriegel, Hans-Peter; Xu, Xiaowei** 1998. "Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications". Data Mining and Knowledge Discovery. Berlin: Springer-Verlag. 2 (2): 169–194. doi:10.1023/A:1009745219419

<!-- slide -->
# DBSCAN

**Выбор параметров**

2. Количество точек в кластере должно быть больше размерности пространства: $minPts \geq D +1$. В качестве отправной точки рекомендуется взять значение $minPts = 2D$

3. Функция вычисления расстояния должна выбираться в соответствии с сущностью точек. Например, если координаты точек заданы широтой и долготой, для вычисления расстояний следует использовать *обратную геодезическую задачу*.

**Sander, Jörg; Ester, Martin; Kriegel, Hans-Peter; Xu, Xiaowei** 1998. "Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications". Data Mining and Knowledge Discovery. Berlin: Springer-Verlag. 2 (2): 169–194. doi:10.1023/A:1009745219419

<!-- slide -->
# OPTICS

OPTICS является модификацией алгоритма DBSCAN, который решает одну из его основных проблем --- невозможность выделять кластеры различной плотности. Точки линейно упорядочиваются таким образом, что близкие друг другу точки становятся соседями в последовательности:

![](OPTICS.svg){width=800px}

<!-- slide -->
# Вычисление центров кластеров

**Возможны различные варианты:**

![](cluster_centers.png){width=500px}

<!-- slide -->
# Критерии качества кластеризации:

**Среднее внутрикластерное расстояние**:

$$
F_0 = \frac{\sum_{i < j} [a_i = a_j] \rho(x_i, x_j)}{\sum_{i < j} [a_i = a_j]} \rightarrow min
$$

**Среднее межкластерное расстояние**:

$$
F_1 = \frac{\sum_{i < j} [a_i \neq a_j] \rho(x_i, x_j)}{\sum_{i < j} [a_i \neq a_j]} \rightarrow max
$$

где $[\cdot]$ --- индикаторная функция в нотации Айверсона:

$$
[P] = \begin{cases}
  1, \text{if } P \text{ is TRUE};\\
  0, \text{if } P \text{ is FALSE}.
\end{cases}
$$

**Отношение пары функционалов**:

$$
F_0/F_1 \rightarrow min
$$
